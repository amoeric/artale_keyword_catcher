#!/usr/bin/env python3
import requests
from bs4 import BeautifulSoup
import json
import time

def analyze_website():
    """åˆ†æ pal.tw ç¶²ç«™çµæ§‹"""
    url = "https://pal.tw/"
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }
    
    try:
        print(f"ğŸ” åˆ†æç¶²ç«™: {url}")
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        print("\nğŸ“‹ ç¶²é åŸºæœ¬è³‡è¨Š:")
        print(f"æ¨™é¡Œ: {soup.title.string if soup.title else 'N/A'}")
        print(f"å…§å®¹é•·åº¦: {len(response.content)} bytes")
        
        # å°‹æ‰¾å¯èƒ½çš„è¨Šæ¯å®¹å™¨
        print("\nğŸ” å°‹æ‰¾å¯èƒ½çš„è¨Šæ¯å…ƒç´ :")
        
        # å¸¸è¦‹çš„è¨Šæ¯å®¹å™¨æ¨™ç±¤å’Œé¡åˆ¥
        possible_containers = [
            'div[class*="message"]',
            'div[class*="chat"]',
            'div[class*="msg"]',
            'div[class*="content"]',
            'li[class*="message"]',
            'li[class*="chat"]',
            'p[class*="message"]',
            '.message',
            '.chat-message',
            '.msg',
            '.content',
            '[data-message]',
            '[data-chat]'
        ]
        
        found_elements = {}
        
        for selector in possible_containers:
            elements = soup.select(selector)
            if elements:
                found_elements[selector] = len(elements)
                print(f"  âœ“ {selector}: {len(elements)} å€‹å…ƒç´ ")
                
                # é¡¯ç¤ºå‰å¹¾å€‹å…ƒç´ çš„å…§å®¹
                for i, elem in enumerate(elements[:3]):
                    text = elem.get_text(strip=True)
                    if text and len(text) > 10:
                        print(f"    ç¯„ä¾‹ {i+1}: {text[:100]}...")
        
        if not found_elements:
            print("  âŒ æœªæ‰¾åˆ°æ˜é¡¯çš„è¨Šæ¯å®¹å™¨")
            
            # å˜—è©¦å°‹æ‰¾æ‰€æœ‰æ–‡å­—å…§å®¹
            print("\nğŸ” å˜—è©¦åˆ†ææ‰€æœ‰æ–‡å­—å…ƒç´ :")
            all_text_elements = soup.find_all(['p', 'div', 'span', 'li'])
            
            for elem in all_text_elements[:10]:
                text = elem.get_text(strip=True)
                if text and len(text) > 20:
                    print(f"  â€¢ {text[:100]}...")
        
        # æª¢æŸ¥æ˜¯å¦æœ‰ JavaScript å‹•æ…‹è¼‰å…¥å…§å®¹
        scripts = soup.find_all('script')
        print(f"\nğŸ“œ JavaScript è…³æœ¬æ•¸é‡: {len(scripts)}")
        
        # å„²å­˜å®Œæ•´çš„ HTML ä¾›åˆ†æ
        with open('website_dump.html', 'w', encoding='utf-8') as f:
            f.write(str(soup.prettify()))
        print("\nğŸ’¾ å®Œæ•´ HTML å·²å„²å­˜åˆ° website_dump.html")
        
        # æª¢æŸ¥æ˜¯å¦æœ‰ WebSocket æˆ–å…¶ä»–å³æ™‚é€šè¨Š
        for script in scripts:
            if script.string:
                script_content = script.string.lower()
                if any(keyword in script_content for keyword in ['websocket', 'socket.io', 'ws://', 'wss://']):
                    print("âš ï¸  æª¢æ¸¬åˆ°å¯èƒ½çš„ WebSocket é€£ç·šï¼Œç¶²ç«™å¯èƒ½ä½¿ç”¨å³æ™‚é€šè¨Š")
                    break
        
        return found_elements
        
    except Exception as e:
        print(f"âŒ åˆ†æå¤±æ•—: {e}")
        return {}

def test_extraction():
    """æ¸¬è©¦è¨Šæ¯æå–"""
    print("\nğŸ§ª æ¸¬è©¦è¨Šæ¯æå–...")
    
    # åŸºæ–¼åˆ†æçµæœèª¿æ•´é¸æ“‡å™¨
    selectors_to_test = [
        'div',  # é€šç”¨ div
        'p',    # æ®µè½
        'li',   # åˆ—è¡¨é …ç›®
        'span', # è¡Œå…§å…ƒç´ 
    ]
    
    url = "https://pal.tw/"
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
    }
    
    try:
        response = requests.get(url, headers=headers, timeout=10)
        soup = BeautifulSoup(response.content, 'html.parser')
        
        for selector in selectors_to_test:
            elements = soup.select(selector)
            messages = []
            
            for elem in elements:
                text = elem.get_text(strip=True)
                # éæ¿¾æœ‰æ„ç¾©çš„æ–‡å­—ï¼ˆé•·åº¦å¤§æ–¼15ï¼ŒåŒ…å«ä¸­æ–‡æˆ–è‹±æ–‡ï¼‰
                if text and len(text) > 15 and any('\u4e00' <= char <= '\u9fff' or char.isalpha() for char in text):
                    messages.append(text)
            
            if messages:
                print(f"\nğŸ“ ä½¿ç”¨é¸æ“‡å™¨ '{selector}' æ‰¾åˆ° {len(messages)} æ¢å¯èƒ½çš„è¨Šæ¯:")
                for i, msg in enumerate(messages[:5]):  # åªé¡¯ç¤ºå‰5æ¢
                    print(f"  {i+1}. {msg[:100]}...")
    
    except Exception as e:
        print(f"âŒ æ¸¬è©¦å¤±æ•—: {e}")

if __name__ == "__main__":
    print("ğŸ¤– pal.tw ç¶²ç«™åˆ†æå·¥å…·")
    print("=" * 50)
    
    # åˆ†æç¶²ç«™çµæ§‹
    found_elements = analyze_website()
    
    # æ¸¬è©¦è¨Šæ¯æå–
    test_extraction()
    
    print("\nâœ… åˆ†æå®Œæˆï¼")
    print("è«‹æª¢æŸ¥ website_dump.html æ–‡ä»¶ä»¥é€²è¡Œæ›´è©³ç´°çš„åˆ†æ") 